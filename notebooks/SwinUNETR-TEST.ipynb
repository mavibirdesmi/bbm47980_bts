{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e81ed10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c4db0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9383e4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dummydataset import get_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33dfce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = get_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d447fdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09d0ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in x.items():\n",
    "    print(f\"{k}: {v.shape} - {v.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2562c160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "slice_num = 100\n",
    "\n",
    "img = x[\"image\"][0][0]\n",
    "\n",
    "label = x[\"label\"][0][0] + 2 * x[\"label\"][0][1]\n",
    "\n",
    "print(f\"image shape: {img.shape}, label shape: {label.shape}\")\n",
    "plt.figure(\"image\", (18, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"image\")\n",
    "plt.imshow(img[:, :, slice_num], cmap=\"gray\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"label\")\n",
    "plt.imshow(label[:, :, slice_num])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b56cf4",
   "metadata": {},
   "source": [
    "### SwinUNETR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd4d46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import Dict, Union\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import argparse\n",
    "from swinunetr import model as smodel\n",
    "from common import miscutils, logutils\n",
    "from common.miscutils import DotConfig\n",
    "from monai.data import decollate_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be2825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logutils.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87c49ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.losses.dice import DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.transforms import AsDiscrete, Activations\n",
    "from monai.inferers import sliding_window_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde3e93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model: torch.nn.Module,\n",
    "    loader: DataLoader,\n",
    "    loss_function: torch.nn.modules.loss._Loss,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    device: torch.device = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Trains the given model for one epoch based on the optimizer given. The training\n",
    "    progress is displayed with a custom progress bar. At the end of the each batch the\n",
    "    mean of the batch loss is displayed within the progress bar.\n",
    "\n",
    "    Args:\n",
    "        model: Model to train.\n",
    "        loader: Data loader.\n",
    "            The batch data should be a dictionary containing \"image\" and \"label\" keys.\n",
    "        loss_function: Loss function to measure the loss during the training.\n",
    "        optimizer: Optimizer to optimize the loss.\n",
    "        epoch: Epoch number. Only used in the progress bar to display the current epoch.\n",
    "        device: Device to load the model and data into. Defaults to None. If set to None\n",
    "            will be set to ``cuda`` if it is available, else will be set to ``cpu``.\n",
    "\n",
    "    Returns:\n",
    "        Trained model.\n",
    "    \"\"\"\n",
    "    if not device:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        device = torch.device(device)\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    dice_metric = DiceMetric(include_background=True, reduction=\"mean_batch\")\n",
    "\n",
    "    epoch_loss = miscutils.AverageMeter()\n",
    "\n",
    "    with logutils.etqdm(loader, epoch=epoch) as pbar:\n",
    "        for batch_data in pbar:\n",
    "            batch_data: Dict[str, torch.Tensor]\n",
    "            image = batch_data[\"image\"].to(device)\n",
    "            label = batch_data[\"label\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits = model(image)\n",
    "            loss: torch.Tensor = loss_function(logits, label)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss = loss.item()\n",
    "\n",
    "            epoch_loss.update(loss, image.size(0))\n",
    "\n",
    "            dice_metric.reset()\n",
    "\n",
    "            metrics = {\n",
    "                \"Mean Train Loss\": loss,\n",
    "            }\n",
    "\n",
    "            pbar.log_metrics(metrics)\n",
    "\n",
    "    history = {\n",
    "        \"Mean Loss\": epoch_loss.avg,\n",
    "    }\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16806e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_epoch(\n",
    "    model: torch.nn.Module,\n",
    "    loader: DataLoader,\n",
    "    loss_function: torch.nn.modules.loss._Loss,\n",
    "    roi_size: int,\n",
    "    sw_batch_size: int,\n",
    "    overlap: int,\n",
    "    labels: DotConfig[str, DotConfig[str, int]],\n",
    "    epoch: int,\n",
    "    device: torch.device = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Evaluates the given model for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model: Model to train.\n",
    "        loader: Data loader.\n",
    "            The batch data should be a dictionary containing \"image\" and \"label\" keys.\n",
    "        loss_function: Loss function to measure the loss during the validation.\n",
    "        roi_size: The spatial window size for inferences.\n",
    "        sw_batch_size: The batch size to run window slices.\n",
    "        overlap: Amount of overlap between scans.\n",
    "        labels: Label key-values configured with DotConfig.\n",
    "            labels should have `BRAIN` and `TUMOR` keys.\n",
    "        epoch: Epoch number. Only used in the progress bar to display the current epoch.\n",
    "        device: Device to load the model and data into. Defaults to None. If set to None\n",
    "            will be set to ``cuda`` if it is available, else will be set to ``cpu``.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If labels does not have either of `BRAIN` and `TUMOR` keys.\n",
    "\n",
    "    Returns:\n",
    "        _description_\n",
    "    \"\"\"\n",
    "    for expected_label in [\"BRAIN\", \"TUMOR\"]:\n",
    "        assert expected_label in labels, f\"labels should have a {expected_label} key!\"\n",
    "\n",
    "    if not device:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        device = torch.device(device)\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    model_inferer = partial(\n",
    "        sliding_window_inference,\n",
    "        roi_size=roi_size,\n",
    "        sw_batch_size=sw_batch_size,\n",
    "        predictor=model,\n",
    "        overlap=overlap,\n",
    "    )\n",
    "\n",
    "    dice_metric = DiceMetric(include_background=True, reduction=\"mean_batch\")\n",
    "\n",
    "    epoch_accuracy = miscutils.AverageMeter()\n",
    "    epoch_loss = miscutils.AverageMeter()\n",
    "\n",
    "    post_sigmoid = Activations(softmax=True)\n",
    "    post_pred = AsDiscrete(argmax=False, threshold=0.5)\n",
    "\n",
    "    with torch.no_grad(), logutils.etqdm(loader, epoch=epoch) as pbar:\n",
    "        for batch_data in pbar:\n",
    "            batch_data: Dict[str, torch.Tensor]\n",
    "            image = batch_data[\"image\"].to(device)\n",
    "            label = batch_data[\"label\"].to(device)\n",
    "\n",
    "            logits = model_inferer(image)\n",
    "\n",
    "            batch_labels = decollate_batch(label)\n",
    "            outputs = decollate_batch(logits)\n",
    "            preds = torch.stack(\n",
    "                [\n",
    "                    post_pred(post_sigmoid(val_pred_tensor))\n",
    "                    for val_pred_tensor in outputs\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            loss: torch.Tensor = loss_function(logits, preds)\n",
    "            loss = loss.item()\n",
    "\n",
    "            epoch_loss.update(loss, image.size(0))\n",
    "\n",
    "            dice_metric.reset()\n",
    "            dice_metric(y_pred=outputs, y=batch_labels)\n",
    "\n",
    "            accuracy = dice_metric.aggregate()\n",
    "            epoch_accuracy.update(accuracy, image.size(0))\n",
    "\n",
    "            num_gt_labels = len(labels)\n",
    "            num_pred_labels = accuracy.numel()\n",
    "            assert num_gt_labels == num_pred_labels, (\n",
    "                f\"Number of labels should match with the number of prediction labels. \"\n",
    "                f\"Found num labels: '{num_gt_labels}' != \"\n",
    "                f\"num prediction labels: '{num_pred_labels}' \"\n",
    "            )\n",
    "\n",
    "            metrics = {\n",
    "                \"Mean Val Brain Acc.\": accuracy[labels.BRAIN],\n",
    "                \"Mean Val Tumor Acc.\": accuracy[labels.TUMOR],\n",
    "                \"Mean Val Loss\": epoch_loss,\n",
    "            }\n",
    "\n",
    "            pbar.log_metrics(metrics)\n",
    "\n",
    "    history = {\n",
    "        \"Mean Brain Acc.\": epoch_accuracy.avg[labels.BRAIN],\n",
    "        \"Mean Tumor Acc.\": epoch_accuracy.avg[labels.TUMOR],\n",
    "        \"Mean Loss\": epoch_loss.avg,\n",
    "    }\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d03749",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = miscutils.load_hyperparameters(\"../swinunetr/hyperparameters.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97785ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "smodel.set_cudnn_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cdf8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smodel.get_model(\n",
    "    img_size=hyperparams.ROI,\n",
    "    in_channels=hyperparams.IN_CHANNELS,\n",
    "    out_channels=hyperparams.OUT_CHANNELS,\n",
    "    feature_size=hyperparams.FEATURE_SIZE,\n",
    "    use_checkpoint=hyperparams.GRADIENT_CHECKPOINT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4edf39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(hyperparams.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a56c94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_loss = DiceLoss(include_background=True, to_onehot_y=False, softmax=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8ba6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=hyperparams.LEARNING_RATE,\n",
    "    weight_decay=hyperparams.WEIGHT_DECAY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfe4b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=hyperparams.EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79be6176",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_dataloader()\n",
    "val_loader = get_dataloader(batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b90054",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329cb6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5df771",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, hyperparams.EPOCHS + 1):\n",
    "    logger.info(f\"Epoch {epoch} is starting.\")\n",
    "\n",
    "    train_history = train_epoch(\n",
    "        model,\n",
    "        loader=train_loader,\n",
    "        loss_function=dice_loss,\n",
    "        optimizer=optimizer,\n",
    "        epoch=epoch,\n",
    "        device=hyperparams.DEVICE,\n",
    "    )\n",
    "\n",
    "    logger.info(f'Mean Train Loss: {train_history[\"Mean Loss\"]}')\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        val_history = val_epoch(\n",
    "            model,\n",
    "            loader=val_loader,\n",
    "            loss_function=dice_loss,\n",
    "            roi_size=hyperparams.ROI,\n",
    "            sw_batch_size=hyperparams.SW_BATCH_SIZE,\n",
    "            overlap=hyperparams.INFER_OVERLAP,\n",
    "            labels=hyperparams.LABELS,\n",
    "            epoch=epoch,\n",
    "            device=hyperparams.DEVICE,\n",
    "        )\n",
    "\n",
    "        val_loss = val_history[\"Mean Loss\"]\n",
    "        val_brain_acc = val_history[\"Mean Brain Acc.\"]\n",
    "        val_tumor_acc = val_history[\"Mean Tumor Acc.\"]\n",
    "\n",
    "        logger.info(\n",
    "            f\"Mean Val Loss: {val_loss} \"\n",
    "            f\"Mean Val Brain Acc.: {val_brain_acc} \"\n",
    "            f\"Mean Val Tumor Acc.: {val_tumor_acc} \"\n",
    "        )\n",
    "\n",
    "    logger.info(f\"Epoch {epoch} finished.\\n\")\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addb3259",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = (val_loader,)\n",
    "loss_function = dice_loss\n",
    "roi_size = hyperparams.ROI\n",
    "sw_batch_size = hyperparams.SW_BATCH_SIZE\n",
    "overlap = hyperparams.INFER_OVERLAP\n",
    "labels = hyperparams.LABELS\n",
    "epoch = epoch\n",
    "device = hyperparams.DEVICE\n",
    "\n",
    "\n",
    "_ = model.eval()\n",
    "\n",
    "model_inferer = partial(\n",
    "    sliding_window_inference,\n",
    "    roi_size=roi_size,\n",
    "    sw_batch_size=sw_batch_size,\n",
    "    predictor=model,\n",
    "    overlap=overlap,\n",
    ")\n",
    "\n",
    "\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean_batch\")\n",
    "\n",
    "epoch_accuracy = miscutils.AverageMeter()\n",
    "epoch_loss = miscutils.AverageMeter\n",
    "\n",
    "post_sigmoid = Activations(softmax=True)\n",
    "post_pred = AsDiscrete(argmax=False, threshold=0.5)\n",
    "\n",
    "with torch.no_grad(), logutils.etqdm(loader, epoch=epoch) as pbar:\n",
    "    for batch_data in pbar:\n",
    "        batch_data: Dict[str, torch.Tensor]\n",
    "        image = batch_data[\"image\"].to(device)\n",
    "        label = batch_data[\"label\"].to(device)\n",
    "\n",
    "        logits = model_inferer(image)\n",
    "\n",
    "        batch_labels = decollate_batch(label)\n",
    "        outputs = decollate_batch(logits)\n",
    "        preds = [\n",
    "            post_pred(post_sigmoid(val_pred_tensor)) for val_pred_tensor in outputs\n",
    "        ]\n",
    "\n",
    "        loss: torch.Tensor = loss_function(logits, preds)\n",
    "        loss = loss.item()\n",
    "\n",
    "        epoch_loss.update(loss, image.size(0))\n",
    "\n",
    "        dice_metric.reset()\n",
    "        dice_metric(y_pred=preds, y=batch_labels)\n",
    "\n",
    "        accuracy = dice_metric.aggregate()\n",
    "        epoch_accuracy.update(accuracy, image.size(0))\n",
    "\n",
    "        num_gt_labels = len(labels)\n",
    "        num_pred_labels = accuracy.numel()\n",
    "        assert num_gt_labels == num_pred_labels, (\n",
    "            f\"Number of labels should match with the number of prediction labels. \"\n",
    "            f\"Found num labels: '{num_gt_labels}' != \"\n",
    "            f\"num prediction labels: '{num_pred_labels}' \"\n",
    "        )\n",
    "\n",
    "        metrics = {\n",
    "            \"Mean Val Brain Acc.\": accuracy[labels.BRAIN],\n",
    "            \"Mean Val Tumor Acc.\": accuracy[labels.TUMOR],\n",
    "            \"Mean Val Loss\": epoch_loss,\n",
    "        }\n",
    "\n",
    "        pbar.log_metrics(metrics)\n",
    "\n",
    "history = {\n",
    "    \"Mean Brain Acc.\": epoch_accuracy.avg[labels.BRAIN],\n",
    "    \"Mean Tumor Acc.\": epoch_accuracy.avg[labels.TUMOR],\n",
    "    \"Mean Loss\": epoch_loss.avg,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42e4b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
